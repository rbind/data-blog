---
title: "An Introduction to Forecasting"
author: Matthias Döring
downloadRmd: false
date: '2018-12-10'
draft: true
description: ""
categories:
  - machine-learning
tags:
    - supervised learning
thumbnail: "/post/machine-learning/forecasting_vs_prediction_avatar.jpg"
---
Previously, I have [discussed the difference between prediction and forecasting](http://127.0.0.1:4321/post/machine-learning/forecasting_vs_prediction/). In short, forecasting makes predictions about the future by relying on past measurements. In this article, I will give an introductiong how ARMA, ARIMA (Box-Jenkins), and ARIMAX models can be used for forecasting of time-series data.

## Preliminaries

Before we can talk about models for time-series data, we have to introduce two concepts.

### The backshift (lag) operator

Given the time series $X = \{X_1, X_2, \ldots \}$, the backshift operator is defined as

\[B X_t = X_{t-1}\,, \forall t > 1\,.\]

This means that one application of the lag operator yields the previous measurement in the time series. Raising the lag operator to a power $k > 0$ performs multiple shifts at once:

\[
\begin{align*}
B^k X_t &= X_{t-k}  \\ 
B^{-k} X& = X_{t+k}
\end{align*}
\]

For example $B^2 X_t$ yields the measurement that was observed two time periods earlier. Instead of $B$, $L$ is used equivalently to indicate the lag operator. 

In R, we can perform the backshift operation using the ```diff``` function. For example:

```{r}
x <- c(1,3,5,10,20)
Bx <- diff(x) # B x
B3x <- diff(x, 3) #B^3 x
message(paste0("x is: ", paste(x, collapse = ","), "\n",
        "Bx is: ", paste(Bx, collapse = ","), "\n",
        "B^3x is: ", paste(B3x, collapse = ",")))
```

### The autocorrelation function

The autocorrelation function (ACF) defines the correlation of a variable $y_t$ with previous measurements $y_{t-1}, \ldots, t_1$ of the same variable (hence the name autocorrelation). To determine the ACF, correlations are calculated for pairs of measurement vector for several lags. The autocorrelation for lag $k$ is defined as:

\[\varphi_{k}:=\operatorname {Corr} (y_{t},y_{t-k})\quad k=0,1,2,\cdots\,.\]

In R, we can manually compute the autocorrelation in the following way:

```{r}
get_autocor <- function(x, lag) {
    x.left <- x[1:(length(x) - lag)]
    x.right <- x[(1+lag):(length(x))]
    autocor <- cor(x.left, x.right)
    return(autocor)
}
get_autocor(x, 1) # correlation of measurements 1 time point apart (lag 1)
get_autocor(x, 2) # correlation of measurements 2 time points apart (lag 2)
```

Since the correlation of later measurements can be based on the correlation to previous measurements, it is often worthwile to consider partial autocorrelation function (pACF). The idea of the pACF is to compute partial correlations where the correlation is conditioned on earlier observations of the variable, more formally:

\[\varphi_{kk}:=\operatorname {Corr} (y_{t},y_{t-k}|y_{t-1},\cdots ,y_{t-k+1})\quad k=0,1,2,\cdots\]

Using the pACF it is possible to identify whether there are actual lagged autocorrelations or whether these autocorrelations are caused by other measurements.

The simplest way to compute and plot the ACF and the pACF is to use the ```acf``` function:

```{r}
par(mfrow = c(1,2))
acf(x) # conventional ACF
pacf(x) # pACF
```

## The ARMA model

ARMA stands for *autoregressive moving average*. ARMA models are only appropriate for stationary processes and have two parameteres:

* **p:** the order of the autoregressive (AR) model 
* **q**: the order of the moving-average (MA) model


The ARMA model can be specified as

\[\hat{y}_t = c + \epsilon_t + \sum_{i=1}^p \phi_i y_{t-i} - \sum_{j=1}^q \theta_j \epsilon_{t-j}\,.\]

with the following variables:

* $c$: the intercept of the model (e.g. the mean)
* $\epsilon_t$: random error (white noise, residual) associated with measurement $t$ with $\epsilon_t \sim N(0, \sigma^2)$.
* $\phi \in \mathbb{R}^p$: a vector of coefficients for the AR terms. In R, these parameters are called *AR1*, *AR2*, and so forth.
* $y_t$: outcome measured at time $t$
* $\theta \in \mathbb{R}^q$: a vector of coefficients for the MA terms. In R, these parameters are called *MA1*, *MA2*, and so forth.
* $\epsilon_t$: noise associated with measurement $t$

Note that the there is no term here for the $d$ parameter, that is, non-stationarity is not allowed because differencing is not performed. To simplify the model formulation, we will introduce the backshift operator now.

To understand how the ARIMA model uses differencing to deal with non-stationarity, we will introduce the lag operator.



### Formulating the ARMA model using the backshift operator

Using the backshift operator, we can formulate the ARMA model in the following way:

\[\left(1 - \sum_{i=1}^p \phi_i B^i \right) y_t = \left(1 - \sum_{j=1}^q \theta_j B^j\right) \epsilon_j\]

By defining $\phi_p(B) = 1 - \sum_{i=1}^p \phi_i B^i$ and $\theta_q(B) = 1 - \sum_{j=1}^q \theta_j B^j$, the ARMA model simplifies to:

\[\phi_p(B) y_t = \theta_q(B) \epsilon_t\,.\]

### Stationary vs non-stationary processes

The ARMA model is only valid for stationary processes. A process is stationary if [its mean and variance are not shifting along the timeline](https://www.r-bloggers.com/stationarity/). Consider the following examples:

```{r}
par(mfrow = c(1,2))
# climate values 
library(tseries)
data(nino)
x <- nino3.4
plot(x, main = "Stationary process")
# Global mean land-ocean temperature deviations
library(astsa)
data(gtemp) 
plot(gtemp, main = "Non-stationary process")
```

The left plot shows a stationary process in which the data behaves similarly throughout all measurements. The right plot shows a non-stationary process in which the mean value is increasing along time.

## The ARIMA model

ARIMA stands for *autoregressive integrated moving average* and is a generalization of the ARMA model. In contrast to ARMA models, ARIMA models are capable of dealing with non-stationary data, that is, time-series where the mean or the variance changes over time. This feature is in the *I* (integrated) of ARIMA: an initial differencing step can eliminate the non-stationarity.

An ARIMA model is specified by three parameters:

* **p:** the order of the autoregressive (AR) model 
* **d:** the degree of differencing
* **q**: the order of the moving-average (MA) model

In the ARIMA model, outcomes are transformed to differences by replacing $y_t$ with differences of the form

\[(1 - B)^d y_t\,.\]

The model is then specified by 

\[\phi_p(B)(1 - B)^d y_t = \theta_q(B) \epsilon_t\,. \]

When $d = 0$, then the model simplifies to the ARMA model since $(1 - B)^0 y_t = y_t$. For other choices of $d$ we obtain backshift polynomials, for example:

\[
\begin{align*}
(1-B)^1 y_t &= y_t - y_{t-1} \\
(1-B)^2 y_t &= (1 - 2B + B^2) y_t = y_t - 2 y_{t-1} + y_{t-2} \\
\end{align*}
\]

### The AR model and $p$

The parameter $p \in \mathbb{N}_0$ specifies the order of the autoregressive model. The term *order* refers to the number of lagged differences that the model considers. For simplicitly, let us assume that $d = 0$ (no differencing). Then, an AR model of order 1 considers only the most recent difference, that is, $B y_t = y_t - y_{t-1}$ via the parameter $\phi_1$. An AR model of order 2 would consider both $B y_t$ and $B^2 y_t$ via $\phi_1$ and $\phi_2$, respectively.

The number of autoregressive terms indicates the extent to which previous measurements influence the current outcome. For example, ARIMA(1,0,0) ($p =1$, $d = 0$, $q = 0$) considers only autoregression with order 1, which means that the outcome is influenced only by the most recent previous measurements. In this case, the model would simplify to

\[\hat{y}_t =  \mu  \epsilon_t +  \phi_1 y_{t-1}\]

#### Impact of autoregression

We can simulate autoregressive processes using the ```arima.sim``` function. Here, the model can be specified by providing the coefficients for the MA and AR terms to be used. To find the impact of autoregression, it is best to use the partial autocorrelation:

```{r, fig.height = 8}
set.seed(5)
par(mfrow = c(2, 2))
# Example for ARIMA(1,0,0)
x <- arima.sim(list(ar = 0.75),
                n = 1000)
plot(x, main = "ARIMA(1,0,0)")
# plot partial acf
acf(x, type = "partial", main = "Partial autocorrelation")
# Example for ARIMA(2,0,0) 
x <- arima.sim(list(ar = c(0.65, 0.3)), 
        n = 1000)
plot(x, main = "ARIMA(2,0,0)")
acf(x, type = "partial", main = "Partial autocorrelation")
```

### The degree of differencing and $d$

The parameter $d \in mathbb{N}_0$ specifies how often the outcomes are differenced via $(1 - B)^d y_t$. In practice, $d$ should be chosen such that we obtain a stationary process. An ARIMA(0,1,0) model would simplifies to the random walk model

\[\hat{y}_t = \mu + \epsilon_t + y_{t-1} \,.\]

This model is random because for every point in time $t$, the mean is simply adjusted by $y_{t-1}$, which leads to random changes of $\hat{y}_t$ over time.

### Impact of differencing

The following example demonstrates the impact of the degree of diferencing:

```{r, fig.height = 8}
par(mfrow = c(2, 2))
# Example for non-stationary process:
x <- arima.sim(list(order = c(0,0,0)), n = 1000)
#stationary.test(x) 
plot(x, main = "ARIMA(0,0,0)")
acf(x, type = "partial", main = "Partial autocorrelation")
# Example for ARIMA(0,1,0)
x <- arima.sim(list(order = c(0,1,0)), n = 1000)
plot(x, main = "ARIMA(0,1,0)")
# note: use diff() to perform differencing before calculating ACF
acf(diff(x), type = "partial", main = "Partial autocorrelation")
```

The greater the degree of differencing $d$ is, the smoother the time-series becomes. 

### The MA model and $q$

The moving average model is specified via $q \in \mathbb{N}_0$. The MA term models the past error, $\epsilon_t$ using coefficients $\theta$. An ARIMA(0,0,1) model simplifies to

\[\hat{y}_t = \mu + \epsilon_t + \theta_1 \epsilon_{t-1} \]

in which the current estimate depends on the residual of the previous measurement.

### Impact of the moving average

To study the impact of the moving average, we should consider the autoregression function:
```{r, fig.height = 8}
par(mfrow = c(2, 2))
# Example for ARIMA(0,0,1)
x <- arima.sim(list(ma = 0.75),
                n = 1000)
plot(x, main = "ARIMA(0,0,1)")
acf(x, main = "Autocorrelation")
# Example for ARIMA(0,0,2)
x <- arima.sim(list(ma = c(0.65, 0.3)), 
        n = 1000)
plot(x, main = "ARIMA(0,0,2)")
acf(x, main = "Autocorrelation")
```

### Choosing between AR and MA terms

To decide which is more appropriate, AR or MA terms, we consider the ACF (autocorrelation function) and PACF (partial ACF). Using these plots we can find

* AR signature: The PACF of the differenced time series displays a sharp cutoff or lag 1 in the PACF is positive. The parameter $p$ is determined by the lag at which the PACF cuts off (last significant autocorrelation).
* MA signature: commonly associated with a negative autocorrelation at lag 1 in the ACF of the differenced time series. The parameter $r$ is determined by the lag at which the ACF cuts off (last significant autocorrelation).

Combinations of AR and MA terms lead to the following time-series data:

```{r, fig.height = 10}
par(mfrow = c(3, 2))
# ARIMA(1,0,1)
x <- arima.sim(list(order = c(1,0,1), ar = 0.8, ma = 0.8), n = 1000)
plot(x, main = "ARIMA(1,0,1)")
acf(x, main = "Autocorrelation")
# ARIMA(2,0,1)
x <- arima.sim(list(order = c(2,0,1), ar = c(0.6, 0.3), ma = 0.8), n = 1000)
plot(x, main = "ARIMA(2,0,1)")
acf(x, main = "Autocorrelation")
# ARIMA(2,0,2)
x <- arima.sim(list(order = c(2,0,2), ar = c(0.6, 0.3), ma = c(0.6, 0.3)), n = 1000)
plot(x, main = "ARIMA(2,0,2)")
acf(x, main = "Autocorrelation")
```

## The SARIMA Model

To model seasonal trends, we need to expand the ARIMA model with the additional parameters $P$, $D$, and $Q$, which correspond to $p$, $d$, and $q$ in the original model:

* **P:** number of seasonal autoregressive (SAR) terms
* **D:** degree of seasonal differencing
* **Q:** number of seasonal moving average (SMA) terms

Accordingly, a seasonal ARIMA (SARIMA) model is denoted by ARIMA(p,d,q)x(P,D,Q)S where $S$ is the period at which the seasonal trend occurs. The additional parameters are included into the ARIMA model in the following way:

\[\Phi_P(B^S) \phi_p(B) (1 - B)^d (1 - B^S) y_t = \Theta_Q(B^S) \theta_q(B) \epsilon_t\,.\]

Here, $\Phi_P$ and $\Theta_Q$ are the coefficients for the seasonal AR and MA components, respectively.





## The ARIMAX model

ARIMAX stands for *autoregressive integrated moving average with exogenous variables*. Here, exogenous variable refer to other covariates $x_t$ that influence the observed time-series values, $y_t$. ARIMAX can be specified by including these $r$ exogenous variables with the coefficient vector $\beta \in \mathbb{R}^r$:

\[\phi_p(B)(1 - B)^d y_t = \beta^T x_t \theta_q(B) \epsilon_t\,. \]

Here, $x_t \in \mathbb{R}^r$ is the $t$-th vector of exogenous features.

## Time-series decomposition

To interpret time-series data, it is useful to decompose the observations $y_t$ into three components:

* Seasonality $S_t$: seasonal trends (e.g. gym memberships rise at the start of the new year)
* Trend $T_t$: overall trends (e.g. the global temperature is increasing)
* Error $\epsilon_t$: unexplained noise

### Additive and multiplicative time-series data

An additive model assumes that the data can be decomposed as

\[y_t = S_t + T_t + \epsilon_t\]

while a multiplicative model assumes that the data can be decomposed as

\[y_t = S_t T_t \epsilon_t\,.\]

To obtain a multiplicative model, we can simply take the logarithm of the $y_t$. The main differences between additive and multiplicative time-series is the following:

* Additive: amplitutdes of seasonal effects are similar in each period.
* Multiplicative: seasonal trend changes with the progression of the time series.

An example for multiplicative time-series data is provided by the following data set:

```{r}
data(AirPassengers)
plot(AirPassengers)
```

As we can see, the effect of the seasonal trend is increasing through the years, which indicates that this data is multiplicative. To remove this effect, we have to take the logarithm of the measurements when we are modeling this data:

```{r}
plot(log(AirPassengers))
```

As we can see, taking the logarithm has equalized the amplitude of the seasonal component along time.

### Example of a decomposition

For example, for the stock market data, the following decomposition can be found:

```{r}
daxData <- EuStockMarkets[, 1] # DAX data
# data do not seem to be multiplicative, use additive decomposition
decomposed <- decompose(daxData, type = "additive")
plot(decomposed)
```

The plot demonstrates the following in the DAX data from 1992 to 1998:

* There is a strong increasing trend in the overall value.
* There is a strong seasonal trend: at the beginng of each year, the stock price is relatively low and reaches its relative maximum at the end of summer.
* The contribution of random noise is negligible, except for 1997 to 1998.


## Forecasting in R

To perform forecasting in R, we will start with a simple ARMA model. ARMA models are only appropriate when the process generating the time-series data is stationary. Otherwise, we have to use ARIMA models. Manually selecting all the parameters of an ARIMA model can be hard. In R, you can automatically fit ARIMA models using the ```auto.arima``` function from the ```forecast``` package. This approach considers reasonable settings for $p$, $d$, and $q$, as well as the sesaonal parameters, $P$, $D$, and $Q$. Note that you should set the parameters ```stepwise``` and ```approximation``` parameters to ```FALSE``` if you are dealing with a single data set. 

### ARIMA model for a stationary process

We will showcase the use of ARMA  using the ```nino``` data from the ```tseries``` package, which gives sea-surface temperatures for the Nino Region 3.4 index. Let us verify that the data are stationary:

```{r}
plot(nino3.4)
```

Since the data is stationary, we can set $d = 0$.

To verify whether there is any seasonal trend, let us decompose the data:

```{r}
nino.components <- decompose(nino3.4)
plot(nino.components)
```

Since this is a stationary process, the overall trend is fluctuating. However, there is a strong seasonal component to the data. Thus, we definitely want to include parameters modeling the seasonal effects. 

#### Seasonal model

Since the seasonal component is very pronounced, we need to include a seasonal model. Since the seasonal trend does not dominate the time-series data, we will set $D = 0$.  Note that the seasonal parameters $(P, D, Q)_S$ are associated with a certain period $S$. Since the seasonal trend in the ```nino``` data is a yearly trend, the parameters refer to $S = 12$ months, i.e. $(P, D, Q)_{12}$. To determine the other parameters for the seasonal model, let us consider the plots for the seasonal component:

```{r}
nino.season <- nino.components$seasonal
#acfpl <- acf(diff(nino.season), main = "pACF", plot = FALSE)
# transform lag from years to months
#acfpl$lag <- acfpl$lag * 12
acf(nino.season, type = "partial")
```

We will use an AR term of order 2 for the seasonal component, that is, we set $P = 2$ and $Q = 0$. Thus, the seasonal model is specified by (2,0,0).

#### Non-seasonal model

For the non-seasonal model, we still need to find $p$ and $q$. 
Next, we will plot the ACF and pACF to identify the values for the AR and MA parameters:

```{r}
# TODO: plot ACF
acfpl <- acf(nino3.4, main = "pACF", type = "partial", plot = FALSE)
# transform lag from years to months
acfpl$lag <- acfpl$lag * 12
plot(acfpl)
#auto.arima(nino3.4)
```

We will set the AR order to $2$ and and the MA order to $1$. This gives the final model: $(2,0,1)x(2,0,0)_{12}$. 

We can fit the model using the ```Arima``` function from the ```forecast``` package. 

```{r}
library(forecast)
order.non.seasonal <- c(2,0,1)
order.seasonal <- c(2,0,0)
A <- Arima(nino3.4, order = order.non.seasonal,
            seasonal = order.seasonal)
# A.best <- auto.arima(nino3.4, stepwise = FALSE, approximation = FALSE) # very different results in plot ... could easily be overfitted!
```

We can now use the model to forecast how the temperatures in the Nino 3.4 region will change in the next year:
```{r}
# to construct a custom plot, we can use the predict function:
forecast <- predict(A, n.ahead = 12) # predict 1 year into the future
library(ggplot2)
plot.df <- rbind(cbind(fortify(nino3.4), sd = 0), cbind(fortify(forecast$pred), sd = as.numeric(forecast$se)))
plot.df$upper <- plot.df$y + plot.df$sd * 1.96
plot.df$lower <- plot.df$y - plot.df$sd * 1.96
ggplot(plot.df, aes(x = x ,y = y)) + 
        geom_line() + geom_ribbon(aes(ymin = lower, ymax = upper), alpha = 0.2) +
        ylab("Temperature") + xlab("Year")
# use the forecast function to use the built-in plotting function:
forecast <- forecast(A, h = 60) # predict 5 years into the future
plot(forecast)
```

### ARIMA model for non-stationary data

To demonstrate an ARIMA mdoel for non-stationary data, we will use the ```gtemp``` data set from the ```astsa``` package. The data set provides yearly measurements of global mean land-ocean temperature deviations.

```{r}
library(astsa)
data(gtemp) 
plot(gtemp)
```

To make the data stationary, we will use $d = 1$:

```{r}
plot(diff(gtemp))
```

Now, the data seems to be stationary. 

Since the measurements were only taken per year, we cannot identify any seasonal characterstics. Thus, we are only concerned with a non-seasonal model in the following. 

```{r}
par(mfrow = c(1,2))
acf(diff(gtemp), main = "ACF")
acf(diff(gtemp), main = "pACF", type = "partial")
#auto.arima(gtemp)
```

Since the first lag's autocorrelation is negative, we will use a moving average model. Thus, we set $p = 0$ and $r = 1$, which leads us to an ARIMA(0,1,1) model. Since the model shows an increase in the mean temperature, we will include a drift term. 

```{r}
A <- Arima(gtemp, order = c(0,1,1), include.drift = TRUE)
#A.best <- auto.arima(gtemp)
```

Let us forecast now:

```{r}
forecast <- forecast(A, h = 30) # predict 30 years into the future
plot(forecast)
```

### ARIMAX on the airquality data set

To showcase the use of an ARIMAX model, we will use ozone data set, [which I had investigated previously](/post/machine-learning/improving_ozone_prediction/).

Let us load the ozone data set and divide it into test and training set. Note that we have ensured that training and test data consist of consecutive temporal measurements. 

```{r}
data(airquality)
ozone <- subset(na.omit(airquality))
set.seed(123)
N.train <- ceiling(0.7 * nrow(ozone))
N.test <- nrow(ozone) - N.train
# ensure to take only subsequent measurements for time-series analysis:
trainset <- seq_len(nrow(ozone))[1:N.train]
testset <- setdiff(seq_len(nrow(ozone)), trainset)
```

Since the data set does not indicate the relative point in time, we will manually create such an annotation:

For this purpose, we will create a new column in the ozone data set, which reflects the relative point in time:

```{r}
# create a time-series object
year <- 1973 # known from data documentation
dates <- as.Date(paste0(year, "-", ozone$Month, "-", ozone$Day))
min.date <- as.numeric(format(min(dates), "%j"))
max.date <- as.numeric(format(max(dates), "%j"))
ozone.ts <- ts(ozone$Ozone, start = min.date, end = max.date, frequency = 1)
ozone.ts <- window(ozone.ts, 121, 231) # deal with repetition due to missing time values
ozone.ts <- ts(ozone$Ozone, start = c(12,5), end = c(12,9))
ozone$t <- seq(start(ozone.ts)[1], end(ozone.ts)[1]) # assumes that measurements are consecutive although they are not
```

```{r}
library(ggplot2)
ggplot(ozone, aes(x = t, y = Ozone)) + geom_line() +
        geom_point()
```

The time-series data seem to be stationary. Let us consider the ACF and pACF plots to see which AR and MA terms we should consider

```{r}
par(mfrow = c(1,2))
acf(ozone.ts)
acf(ozone.ts, type = "partial")
```

The autocorrelations suggests that there is actually no time trend in the data, that is, we find that we need an ARIMA(0,0,0) model. Since an ARIMAX model with parameters (0,0,0) does not have a benefit over a conventional linear regression model, we can assume that the temporal trend in the ozone data is not sufficient to improve the prediction of ozone levels. We can verify this:

```{r}
# ARIMAX(0,0,0) model
library(TSA)
features <- c("Solar.R", "Wind", "Temp") # exogenous features
A <- arimax(x = ozone$Ozone[trainset], 
       xreg = ozone[trainset,features],
        order = c(0,0,0))
preds.temporal <- predict(A, newxreg = ozone[testset, features])
# Weighted negative binomial model
library(MASS)
get.weights <- function(ozone) {
    z.scores <- (ozone$Ozone - mean(ozone$Ozone)) / sd(ozone$Ozone)
    weights <- exp(z.scores)
    weights <- weights / mean(weights) # normalize to mean 1
    return(weights)
}
weights <- get.weights(ozone)
# train weighted negative binomial model
model.nb <- glm.nb(Ozone ~ Solar.R + Temp + Wind, data = ozone, subset = trainset, weights = weights)
preds.nb <- predict(model.nb, newdata = ozone[testset,], type = "response")
# Performance:
Rsquared.linear <- cor(preds.nb, ozone[testset, "Ozone"])^2
Rsquared.temporal <- cor(preds.temporal$pred, ozone[testset, "Ozone"])^2
print(Rsquared.linear)
print(Rsquared.temporal)
```

As we can see the linear model with a negative binomial likelihood outperforms the ARIMAX model.

### ARIMAX on the airquality data set

#I have obtained the [icecream data set](https://www.datascienceblog.net/data-sets/Icecream.csv) from [R-exercises](https://www.r-bloggers.com/forecasting-arimax-model-exercises-part-5/). 

```{r}
library(Ecdat)
data(Icecream)
```

The icecream data contains the following variables:

* **cons:** The ice cream consumption in pints per capita.
* **income:**: The average weekly family income in USD.
* **price:** The price of ice cream per pint.
* **temp:** The average temperature in Fahrenheit.

The measurements are four-weekly observations from 1951-03-18 to 1953-07-11.

We will model *cons*, the ice cream consumption as a time series and use *income*, *price*, and *average* as exogenous variables.

```{r}
# create a time-series object
library(lubridate)
wk <- week(c(as.Date("1951-03-18"), as.Date("1953-07-11")))
months <- c(seq(3,12), seq(1,12), seq(1,7))
wks <- c(seq(wk[1], 52, 4), seq(1, 52, 4), seq(1, 52, 4))
ice.ts <- ts(Icecream$cons, start = c(1951, 3), end = c(1953, 6), frequency = 52/4)
plot(ice.ts)
plot(decompose(ice.ts))
pacf(ice.ts)
```

So, there are two trends in the data:

1. Overall, icecream consumption has increased considerably between 1951 and 1953.
2. Ice cream sales are peaking in the summer.

Without exogenous variables, we would probably fit an ARIMA(1,0,0)(1,0,0) model. However, since we have the temperature and the income, this can probably explain the seasonal trend well:

```{r}
plot(ice$income) # explains the overall trend
plot(ice$temp) # explains the seasonal trend
```

So, we do not need a seasonal model anymore and instead use an ARIMA(1,0,0) model for the forecasting.

```{r}
train <- 1:20
test <- 21:30
A <- Arima(window(ice.ts, c(1951,3), c(1951, 3 + (20-1))),
        xreg = ice[train, c("income", "temp")],
        order = c(1,0,0))
preds <- forecast(A, xreg = ice[test, c("income", "temp")]) # TODO: how to forecast when we don't have additional data for future points?
plot(preds) # forecast in blue
lines(window(ice.ts, c(1951, 22), c(1951, 30))) # actual values in black
# contrast ARIMAX with ARIMA
A.season <- Arima(window(ice.ts, c(1951,3), c(1951, 3 + (20-1))),
        order = c(1,0,0),
        season = c(1,0,0))
preds <- forecast(A.season, h = 12)
plot(preds) # forecast in blue
lines(window(ice.ts, c(1951, 22), c(1951, 30))) # actual values in black

```
## References
https://otexts.org/fpp2/backshift.html
http://people.duke.edu/~rnau/411arim.htm
https://www.r-bloggers.com/forecasting-arimax-model-exercises-part-5/
https://forecasters.org/wp-content/uploads/gravity_forms/7-2a51b93047891f1ec3608bdbd77ca58d/2013/07/Kongcharoen_Chaleampong_ISF2013.pdf
https://onlinecourses.science.psu.edu/stat510/node/67/
https://support.minitab.com/en-us/minitab/18/help-and-how-to/modeling-statistics/time-series/how-to/partial-autocorrelation/interpret-the-results/partial-autocorrelation-function-pacf/
https://datascienceplus.com/time-series-analysis-building-a-model-on-non-stationary-time-series/
https://stat.ethz.ch/education/semesters/ss2014/atsa/Scriptum_v140523.pdf
https://rpubs.com/jamesokamoto/300442
