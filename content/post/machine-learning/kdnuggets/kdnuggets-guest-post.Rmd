---
title: "KDNuggets Guest Post: Machine Learning Through the Years"
author: Matthias Döring
date: '2018-11-26'
description: "TODO"
draft: true
categories:
  - machine-learning
---
The field of machine learning has gone through enormous changes in the last decades. For example, support vector machines and random forests were developed in the mid 1990s. However, some methods that are still used nowadays have been around for quite a long time. For example, the concept of least squares was already proposed in the early 19th century by Legendre and Gauss. Other approaches such as neural networks, which have been described already in the 1950s, have been steadily advanced through the years.

## Machine learning models through the years

A question that is often asked is: what is the best model? Answering this question is not really possible as the choice of the model is inherently linked to the data that is being modeled. A question that is easier to answer is: what is the most popular model? Here, I investigate this question in a data-driven manner. More precisely, I will define popularity in a frequentist manner. The assumption of this analysis is that a method that is associated with a greater number of
publications is more popular. Obviously, this is not necessarily true because publications can also indicate that an algorithm is criticized or that there is a lot of room for improvement. However, overall, there should be a high correlation between the two concepts.

For this piece, I heavily relied on data scraping from Google Scholar. Google Scholar searches in the titles and abstracts of scientific publications. To identify the number of publications associated with individual supervised learning approaches, I determined the number of Google Scholar hits for each method between 1950 and 2018. Since Google blocks automated requests, I found [helpful advice from ScrapeHero](https://www.scrapehero.com/how-to-fake-and-rotate-user-agents-using-python-3/) to circumvent this problem. The following 12 models were considered: neural networks, support vector machines, random forests, decision trees, linear regression, logistic regression, poisson regression, ridge regression, lasso regression, nearest neighbors, linear discriminant analysis, log-linear models.

For lasso regression, the terms *lasso regression* and *lasso model* were considered. For nearest neighbors, the terms *nearest neighbor* and *nearest neighbour* were considered. The resulting data set indicates the [number of publications associated with each of the twelve supervised models from 1950 until now](result.csv).

To analyze the data, I will differentiate between two periods: the early days of machine learning (1950 to 1980), in which few models were available, and the formative years (1980 until now), in which general interest in machine learning surged and many new models were developed.

```{r}
df <- read.csv("result.csv")
# plot all and for all time
library(ggplot2)
ggplot(df, aes(x = Year, y = Count, color = Model)) + 
        geom_point() + geom_line() + ggtitle("Overall")
# select only the top-k models overall (from 1950 to now)
k <- 6
library(plyr)
total.df <- ddply(df, "Model", summarize, Total = sum(Count))
o <- order(total.df$Total, decreasing = TRUE)
top.models <- total.df[o, "Model"][1:k]
plot.df <- df[df$Model %in% top.models, ]
library(ggplot2)
p.df.early <- plot.df[plot.df$Year >= 1950 & plot.df$Year < 1980, ]
ggplot(p.df.early, aes(x = Year, y = Count, color = Model)) + 
        geom_point() + geom_line() + ggtitle("Early days of machine learning")
# zoom into the plot, starting from the 80s
p.df.late <- plot.df[plot.df$Year >= 1980,]
ggplot(p.df.late, aes(x = Year, y = Count, color = Model)) + 
        geom_point() + geom_line() + ggtitle("Formative years of machine learning")
```

We see that throughout the years, linear regression was the most mentioned technique. The popularity of logistic regression and neural networks took off in the early 2000s, with the use of logistic regression peaking in the early 2010s and neural networks peaking around 2014. All three methods, linear regression, logistic regression, and neural networks are characterized by similarly steep increases in their usage. Of course, linear regression became popular much earlier than the other two methods. Interestingly, however, the interest in logistic regression and neural networks show highly similar temporal growth trends from 2000 to 2010. 

Overall, neural networks are the method with the greatest increase in popularity. In the 1980s, neural networks overtook logistic regression, in the 1990s neural networks overtook nearest neighbors, and in 2016, neural networks even overtook linear regression. However, the usage of these three methods is also characterized by similarly sharp drops in the last three years. 

Other approaches such as nearest neighbor, decision trees, and SVMs showed more steady growth patterns. While the popularity of these methods has not noticeably declined in recent years, the number of papers based on these methods stopped increasing in the last three years. Among the three methods, nearest neighbor algorithm is the most popular, which is probably also due to the fact that it was already established in 1967, while decision trees were established in the 1980s and SVMs in 1995. Looking at the data, it is interesting to see that SVMs managed to become more popular than decision trees in 2010.

Overall, the data demonstrate the breakthrough of supervised learning starting rougly from 2000s. We also see that neural networks have become the dominant machine learning method in the 2010s. The data, however, suggest that the popularity of neural networks is currently actually decreasing. This could be interpreted as the bursting of a bubble. The use of neural networks has led to breakthroughs in machine learning applications such as image recognition (ImageNet, 2009), face recognition (DeepFace, 2014), and gaming (AlphaGo, 2016). Therefore, neural networks have become immensely popular. Now, however, it could be that people realize that deep neural networks are not the solution to all problems because it is not possible to generate hundreds of thousands of data points for every prediction problem. 


## Popularity of supervised learning models: overall and  across different fields

Another question I was interested in was whether there was a difference in the frequencies with which different machine learning models are mentioned in publications from different communities. For this purpose, I counted the number of search results from Google Scholar to determine the overall frequency. For community-specfic frequencies, I performed the same queries on dplb (computer science) and on PubMed (biomedical sciences).

```{r}
df <- read.csv("ml_model_publications.csv")
library(reshape2)
df.m <- melt(df, "Source")
# get percentages: values are not comparable otherwise
library(dplyr)
df.m <- group_by(df.m, Source) %>% mutate(percent = value/sum(value, na.rm = TRUE))
colnames(df.m) <- c("Source", "Model", "Count", "Percentage")
# sort columns by overall usage
overall.idx <- which(df.m$Source == "Overall (Google Scholar)")
o <- order(df.m$Count[overall.idx], decreasing = TRUE)
df.m$Model <- factor(df.m$Model, levels = levels(df.m$Model)[o])
ggplot(df.m, aes(x = Model, y = Percentage, fill = Source)) + geom_bar(stat = "identity") +
    theme(axis.text.x = element_text(angle = 90, hjust = 1, vjust = 0.5)) + 
    scale_y_continuous(labels = scales::percent_format())
# top 6 models by source
n <- 6
for (source in unique(df.m$Source)) {
    idx <- which(df.m$Source == source)
    o <- order(df.m$Percentage[idx], decreasing = TRUE)
    top.df <- df.m[idx[o[1:n]], ]
    print(top.df)
}
```

Overall (according to Google Scholar), these are the 6 most frequently referenced supervised models (from a total of 11,788,100 papers mentioning the considered models):

1. Linear regression with 3,580,000 (30.4%) papers
2. Logistic regression with 2,330,000 (19.8%) papers
3. Neural networks with 1,750,000 (14.8%) papers
4. Nearest neighbors with 1,738,000 (14.7%) papers
5. Decision trees with 875,000 (7.4%) papers
6. Support vector machines with 684,000 (5.8%) papers

The overrepresentation of methods such as linear regression and logistic regression is not surprising because they have been around for a much longer time than other methods. Although support vector machines (SVMs) are one of the more recent methods, they are more popular than other methods that have been developed at the same time (e.g. random forests). 

Looking at the numbers in the biomedical and computer sciences, there are critical differences.

Biomedical:

1. Logistic regression: 229,956 (54.2%) papers
2. Linear regression: 84,850 (20%) papers
3. Cox regression: 38,801 (9.1%) papers
4. Neural networks: 23,883 (5.6%) papers
5. Poisson regression: 12,978 (3.1%) papers
6. Support vector machines: 11,061 (2.6%) papers

Computer science:

1. Neural networks: 63,695 (75%) papers
2. Support vector machines: 7,750 (8.9%) papers
3. Decision trees: 4,074 (4.8%) papers
4. Nearest neighbors: 3,839 (4.5%) papers
5. Random forests: 1,863 (2.2%) papers
6. Linear regression: 1,643 (1.9%) papers

Where do these considerable differences come from? The popularity of logistic regression in the PubMed data probably originates from the large number of clinical studies that are contained in the data. There, the categorical outcomes (e.g. treatment success) is often analyzed using logistic regression because it can be used for interpreting the impact of the features on the outcome. In the biomedical data, we see an overrepresentation of non-linear models. This is probably due to two
reasons. First, in medical settings, the number of samples is often small such that it is infeasible to obtain good fits with complex models. Second, interpreting the results is critical for medical applications, which is why linear models are advantageous over non-linear methods, which are typically harder to interpret. It is also remarkable that Cox regression, a model for Kaplan-Meier survival data, is so frequently used (9.1% of papers).

The computer science publications show an opposing trend: the majority of publications concerns recent, non-linear methods. Particularly the number of publications about neural networks, which make up three quarters of all retrieved publications, is astounding. Although SVMs are the second most frequently mentioned method, they are far behind (8.9% of papers). In some way, this is not surprising considering that there have been much more advances in neural networks than in SVMs,
which is likely a byproduct of a greater focus on researching neural networks.

These data also demonstrate why there is such a disconnect between machine learning researchers and persons that apply machine learning. Research is heavily focused on individual, state-of-the-art methods such as convolutional neural networks. Appliers of machine learning, however, still heavily rely on more traditional approaches such as linear or logistic regression.

--
References:
 A.M. Legendre. Nouvelles méthodes pour la détermination des orbites des comètes, Firmin Didot, Paris, 1805. “Sur la Méthode des moindres quarrés” appears as an appendix.
C.F. Gauss. Theoria Motus Corporum Coelestium in Sectionibus Conicis Solem Ambientum. (1809)
